"""
Scraper pour HCP.ma - Haut Commissariat au Plan
================================================
Scrape les indicateurs de sant√© et personnes √† besoins sp√©cifiques.

Installation:
pip install requests beautifulsoup4 pandas openpyxl lxml
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import os
from datetime import datetime
from urllib.parse import urljoin
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class HCPScraper:
    """Scraper pour les indicateurs de sant√© du HCP"""
    
    def __init__(self, output_dir='hcp_sante'):
        self.base_url = "https://www.hcp.ma"
        self.output_dir = output_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Cr√©er dossiers
        os.makedirs(f'{output_dir}/indicateurs_sante', exist_ok=True)
        os.makedirs(f'{output_dir}/handicap', exist_ok=True)
        os.makedirs(f'{output_dir}/nutrition', exist_ok=True)
        os.makedirs(f'{output_dir}/publications', exist_ok=True)
        os.makedirs(f'{output_dir}/metadata', exist_ok=True)
        
        logging.info(f"‚úì Dossiers cr√©√©s dans: {output_dir}")
    
    def scrape_indicateurs_sante(self):
        """Scrape les indicateurs sant√© et personnes √† besoins sp√©cifiques"""
        url = f"{self.base_url}/Indicateurs-Sante-et-personnes-a-besoins-specifiques_r591.html"
        logging.info(f"Scraping indicateurs sant√©: {url}")
        
        indicateurs = []
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher tous les tableaux
            tables = soup.find_all('table')
            logging.info(f"  {len(tables)} tableaux trouv√©s")
            
            for i, table in enumerate(tables, 1):
                try:
                    # Parser le tableau
                    df = pd.read_html(str(table))[0]
                    
                    if len(df) > 0:
                        # Sauvegarder
                        filename_base = f'{self.output_dir}/indicateurs_sante/indicateur_{i}'
                        df.to_csv(f'{filename_base}.csv', index=False, encoding='utf-8-sig')
                        df.to_excel(f'{filename_base}.xlsx', index=False, engine='openpyxl')
                        
                        logging.info(f"  ‚úì Tableau {i}: {len(df)} lignes √ó {len(df.columns)} colonnes")
                        
                        indicateurs.append({
                            'table_id': i,
                            'lignes': len(df),
                            'colonnes': len(df.columns),
                            'colonnes_noms': list(df.columns),
                            'fichier': filename_base
                        })
                        
                except Exception as e:
                    logging.warning(f"  ‚ö† Erreur table {i}: {e}")
                    continue
            
            # Chercher aussi les liens vers documents Excel/PDF
            links = soup.find_all('a', href=True)
            documents = []
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if any(ext in href.lower() for ext in ['.xls', '.xlsx', '.pdf']):
                    full_url = urljoin(self.base_url, href)
                    documents.append({
                        'titre': text,
                        'url': full_url,
                        'type': href.split('.')[-1].upper()
                    })
                    logging.info(f"  üìÑ Document trouv√©: {text}")
            
            # Sauvegarder m√©tadonn√©es
            metadata = {
                'url': url,
                'date_scraping': datetime.now().isoformat(),
                'tableaux': indicateurs,
                'documents': documents
            }
            
            with open(f'{self.output_dir}/metadata/indicateurs_sante.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logging.info(f"‚úì {len(indicateurs)} tableaux extraits")
            return indicateurs
            
        except Exception as e:
            logging.error(f"Erreur scraping indicateurs: {e}")
            return indicateurs
    
    def scrape_nutrition_sante(self):
        """Scrape les indicateurs nutrition & sant√©"""
        url = f"{self.base_url}/Indicateurs-Nutrition-sante_r486.html"
        logging.info(f"Scraping nutrition & sant√©: {url}")
        
        tables_data = []
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            tables = soup.find_all('table')
            logging.info(f"  {len(tables)} tableaux trouv√©s")
            
            for i, table in enumerate(tables, 1):
                try:
                    df = pd.read_html(str(table))[0]
                    
                    if len(df) > 0:
                        filename_base = f'{self.output_dir}/nutrition/nutrition_{i}'
                        df.to_csv(f'{filename_base}.csv', index=False, encoding='utf-8-sig')
                        df.to_excel(f'{filename_base}.xlsx', index=False, engine='openpyxl')
                        
                        logging.info(f"  ‚úì Tableau {i}: {len(df)} lignes")
                        tables_data.append({'id': i, 'lignes': len(df)})
                        
                except Exception as e:
                    logging.warning(f"  ‚ö† Erreur table {i}: {e}")
                    continue
            
            logging.info(f"‚úì {len(tables_data)} tableaux nutrition extraits")
            return tables_data
            
        except Exception as e:
            logging.error(f"Erreur scraping nutrition: {e}")
            return tables_data
    
    def scrape_publications_sante(self):
        """Scrape les publications sur la sant√©"""
        url = f"{self.base_url}/Sante-et-personnes-a-besoins-specifiques_r589.html"
        logging.info(f"Scraping publications sant√©: {url}")
        
        publications = []
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher liens vers publications
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # Filtrer les publications pertinentes
                if any(keyword in text.lower() for keyword in 
                      ['covid', 'sant√©', 'sante', 'handicap', 'couverture m√©dicale', '√©pid√©mie']):
                    
                    full_url = urljoin(self.base_url, href)
                    
                    pub = {
                        'titre': text,
                        'url': full_url,
                        'type': 'publication'
                    }
                    
                    publications.append(pub)
                    logging.info(f"  üì∞ Publication: {text[:60]}...")
                    
                    # Si PDF, t√©l√©charger
                    if href.lower().endswith('.pdf'):
                        self._download_file(full_url, text, 'publications')
            
            # Sauvegarder liste
            with open(f'{self.output_dir}/metadata/publications.json', 'w', encoding='utf-8') as f:
                json.dump(publications, f, ensure_ascii=False, indent=2)
            
            logging.info(f"‚úì {len(publications)} publications trouv√©es")
            return publications
            
        except Exception as e:
            logging.error(f"Erreur scraping publications: {e}")
            return publications
    
    def scrape_indicateurs_sociaux(self):
        """T√©l√©charge les indicateurs sociaux (PDF)"""
        # URLs des rapports annuels
        urls = [
            "https://casainvest.ma/sites/default/files/Les indicateurs sociaux du Maroc HCP 2023.pdf",
            "https://marocpme.gov.ma/wp-content/uploads/2024/04/Les-indicateurs-sociaux-du-Maroc-Edition-2024.pdf"
        ]
        
        indicateurs = []
        
        for url in urls:
            try:
                logging.info(f"T√©l√©chargement: {url}")
                year = '2023' if '2023' in url else '2024'
                filename = f"Indicateurs_Sociaux_Maroc_{year}.pdf"
                
                self._download_file(url, filename, 'publications')
                indicateurs.append({'annee': year, 'url': url})
                
            except Exception as e:
                logging.warning(f"Erreur t√©l√©chargement {url}: {e}")
                continue
        
        return indicateurs
    
    def _download_file(self, url, titre, subfolder):
        """T√©l√©charge un fichier"""
        try:
            safe_name = "".join(c for c in titre if c.isalnum() or c in (' ', '-', '_'))[:80]
            
            # D√©terminer extension
            if url.lower().endswith('.pdf'):
                ext = '.pdf'
            elif url.lower().endswith('.xlsx'):
                ext = '.xlsx'
            elif url.lower().endswith('.xls'):
                ext = '.xls'
            else:
                ext = ''
            
            filename = f'{self.output_dir}/{subfolder}/{safe_name}{ext}'
            
            if os.path.exists(filename):
                logging.info(f"    ‚äò D√©j√† t√©l√©charg√©")
                return filename
            
            logging.info(f"    ‚Üì T√©l√©chargement...")
            response = self.session.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logging.info(f"    ‚úì Sauvegard√©")
            time.sleep(2)
            return filename
            
        except Exception as e:
            logging.error(f"    ‚úó Erreur t√©l√©chargement: {e}")
            return None
    
    def scrape_all(self):
        """Lance le scraping complet"""
        logging.info("="*60)
        logging.info("D√âMARRAGE SCRAPING HCP - SANT√â")
        logging.info("="*60)
        
        rapport = {
            'date': datetime.now().isoformat(),
            'source': 'HCP - Haut Commissariat au Plan',
            'indicateurs_sante': 0,
            'indicateurs_nutrition': 0,
            'publications': 0,
            'rapports_annuels': 0
        }
        
        # 1. Indicateurs sant√©
        logging.info("\n1. Scraping indicateurs sant√©...")
        indicateurs = self.scrape_indicateurs_sante()
        rapport['indicateurs_sante'] = len(indicateurs)
        
        # 2. Nutrition
        logging.info("\n2. Scraping indicateurs nutrition...")
        nutrition = self.scrape_nutrition_sante()
        rapport['indicateurs_nutrition'] = len(nutrition)
        
        # 3. Publications
        logging.info("\n3. Scraping publications...")
        publications = self.scrape_publications_sante()
        rapport['publications'] = len(publications)
        
        # 4. Rapports annuels
        logging.info("\n4. T√©l√©chargement rapports annuels...")
        rapports = self.scrape_indicateurs_sociaux()
        rapport['rapports_annuels'] = len(rapports)
        
        # Sauvegarder rapport
        with open(f'{self.output_dir}/rapport_hcp.json', 'w', encoding='utf-8') as f:
            json.dump(rapport, f, ensure_ascii=False, indent=2)
        
        logging.info("\n" + "="*60)
        logging.info("SCRAPING HCP TERMIN√â!")
        logging.info("="*60)
        logging.info(f"Indicateurs sant√©: {rapport['indicateurs_sante']}")
        logging.info(f"Indicateurs nutrition: {rapport['indicateurs_nutrition']}")
        logging.info(f"Publications: {rapport['publications']}")
        logging.info(f"Rapports annuels: {rapport['rapports_annuels']}")
        logging.info(f"\nR√©sultats dans: {self.output_dir}/")
        logging.info("="*60)
        
        return rapport


if __name__ == "__main__":
    scraper = HCPScraper()
    rapport = scraper.scrape_all()