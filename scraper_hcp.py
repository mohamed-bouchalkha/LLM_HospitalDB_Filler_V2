"""
Scraper pour HCP.ma - VERSION ADAPT√âE
======================================
T√©l√©charge les Indicateurs Sociaux du Maroc et autres publications officielles.

Le HCP publie des fichiers Excel t√©l√©chargeables avec toutes les donn√©es de sant√©.

Installation:
pip install requests beautifulsoup4 pandas openpyxl lxml
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import os
from datetime import datetime
from urllib.parse import urljoin
import logging
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class HCPScraper:
    """Scraper pour les publications du HCP"""
    
    def __init__(self, output_dir='hcp_donnees'):
        self.base_url = "https://www.hcp.ma"
        self.output_dir = output_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Cr√©er dossiers
        os.makedirs(f'{output_dir}/indicateurs_sociaux', exist_ok=True)
        os.makedirs(f'{output_dir}/sante', exist_ok=True)
        os.makedirs(f'{output_dir}/demographie', exist_ok=True)
        os.makedirs(f'{output_dir}/publications_pdf', exist_ok=True)
        os.makedirs(f'{output_dir}/fichiers_excel', exist_ok=True)
        os.makedirs(f'{output_dir}/metadata', exist_ok=True)
        
        logging.info(f"‚úì Dossiers cr√©√©s dans: {output_dir}")
    
    def telecharger_indicateurs_sociaux(self):
        """T√©l√©charge les Indicateurs Sociaux du Maroc (fichiers Excel)"""
        url = f"{self.base_url}/downloads/Les-indicateurs-sociaux_t22430.html"
        logging.info(f"Scraping Indicateurs Sociaux: {url}")
        
        fichiers = []
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher tous les liens vers fichiers Excel et PDF
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # Filtrer fichiers Excel et PDF
                if any(ext in href.lower() for ext in ['.xls', '.xlsx', '.pdf', '.zip']):
                    full_url = urljoin(self.base_url, href)
                    
                    # D√©terminer ann√©e
                    year = self._extract_year(text + href)
                    
                    # T√©l√©charger
                    if '.xls' in href.lower():
                        result = self._download_file(full_url, text, 'fichiers_excel')
                    else:
                        result = self._download_file(full_url, text, 'indicateurs_sociaux')
                    
                    if result:
                        fichiers.append({
                            'titre': text,
                            'url': full_url,
                            'annee': year,
                            'fichier': result
                        })
            
            logging.info(f"‚úì {len(fichiers)} Indicateurs Sociaux t√©l√©charg√©s")
            return fichiers
            
        except Exception as e:
            logging.error(f"Erreur t√©l√©chargement Indicateurs Sociaux: {e}")
            return fichiers
    
    def telecharger_annuaire_statistique(self):
        """T√©l√©charge l'Annuaire Statistique du Maroc (Excel)"""
        url = f"{self.base_url}/downloads/Annuaire-Statistique-du-Maroc-format-Excel_t22392.html"
        logging.info(f"T√©l√©chargement Annuaire Statistique: {url}")
        
        fichiers = []
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher liens Excel
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if '.xls' in href.lower() or '.zip' in href.lower():
                    full_url = urljoin(self.base_url, href)
                    result = self._download_file(full_url, text, 'fichiers_excel')
                    
                    if result:
                        fichiers.append({
                            'titre': text,
                            'url': full_url,
                            'fichier': result
                        })
            
            logging.info(f"‚úì {len(fichiers)} Annuaires t√©l√©charg√©s")
            return fichiers
            
        except Exception as e:
            logging.error(f"Erreur Annuaire: {e}")
            return fichiers
    
    def scrape_page_telechargements(self):
        """Scrape la page principale de t√©l√©chargements"""
        url = f"{self.base_url}/downloads/"
        logging.info(f"Exploration page t√©l√©chargements: {url}")
        
        publications = {
            'sante': [],
            'demographie': [],
            'autres': []
        }
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher tous les liens
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                full_url = urljoin(self.base_url, href)
                
                # Cat√©goriser par mot-cl√©
                if any(kw in text.lower() for kw in ['sant√©', 'sante', 'handicap', 'mortalit√©', 'nutrition']):
                    publications['sante'].append({
                        'titre': text,
                        'url': full_url
                    })
                    logging.info(f"  üìä Sant√©: {text[:60]}...")
                    
                elif any(kw in text.lower() for kw in ['d√©mographie', 'population', 'recensement']):
                    publications['demographie'].append({
                        'titre': text,
                        'url': full_url
                    })
                    
                # Si PDF ou Excel li√© √† sant√©, t√©l√©charger
                if (any(kw in text.lower() for kw in ['sant√©', 'sante', 'indicateurs sociaux']) and 
                    (href.endswith('.pdf') or '.xls' in href.lower())):
                    
                    if '.pdf' in href.lower():
                        self._download_file(full_url, text, 'publications_pdf')
                    else:
                        self._download_file(full_url, text, 'fichiers_excel')
            
            # Sauvegarder m√©tadonn√©es
            with open(f'{self.output_dir}/metadata/publications_trouvees.json', 'w', encoding='utf-8') as f:
                json.dump(publications, f, ensure_ascii=False, indent=2)
            
            logging.info(f"‚úì Publications sant√©: {len(publications['sante'])}")
            logging.info(f"‚úì Publications d√©mographie: {len(publications['demographie'])}")
            
            return publications
            
        except Exception as e:
            logging.error(f"Erreur exploration t√©l√©chargements: {e}")
            return publications
    
    def telecharger_publications_specifiques(self):
        """T√©l√©charge des publications sp√©cifiques importantes"""
        logging.info("T√©l√©chargement publications sp√©cifiques...")
        
        # URLs directes identifi√©es
        urls_importantes = [
            # Indicateurs Sociaux (Excel)
            f"{self.base_url}/file/241136/",  # Indicateurs Sociaux 2023
            f"{self.base_url}/file/241135/",  # Indicateurs Sociaux 2022
            
            # Publications sant√©
            f"{self.base_url}/file/231571/",  # Indicateurs sant√© reproductive
            f"{self.base_url}/file/231570/",  # Indicateurs mortalit√©
        ]
        
        fichiers = []
        
        for url in urls_importantes:
            try:
                logging.info(f"  T√©l√©chargement: {url}")
                response = self.session.get(url, timeout=60, stream=True)
                response.raise_for_status()
                
                # D√©terminer nom fichier depuis headers
                content_disp = response.headers.get('content-disposition', '')
                if 'filename=' in content_disp:
                    filename = content_disp.split('filename=')[1].strip('"')
                else:
                    filename = f"publication_{url.split('/')[-2]}"
                
                # D√©terminer extension
                content_type = response.headers.get('content-type', '')
                if 'excel' in content_type or 'spreadsheet' in content_type:
                    if not filename.endswith(('.xls', '.xlsx')):
                        filename += '.xlsx'
                    folder = 'fichiers_excel'
                elif 'pdf' in content_type:
                    if not filename.endswith('.pdf'):
                        filename += '.pdf'
                    folder = 'publications_pdf'
                else:
                    folder = 'sante'
                
                filepath = f'{self.output_dir}/{folder}/{filename}'
                
                # Sauvegarder
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                logging.info(f"    ‚úì Sauvegard√©: {filename}")
                fichiers.append(filepath)
                time.sleep(2)
                
            except Exception as e:
                logging.warning(f"    ‚ö† Erreur {url}: {e}")
        
        logging.info(f"‚úì {len(fichiers)} publications sp√©cifiques t√©l√©charg√©es")
        return fichiers
    
    def _extract_year(self, text):
        """Extrait l'ann√©e d'un texte"""
        years = re.findall(r'20\d{2}', text)
        return years[-1] if years else None
    
    def _make_safe_filename(self, filename):
        """Cr√©e un nom de fichier s√©curis√©"""
        safe = re.sub(r'[<>:"/\\|?*]', '_', filename)
        safe = "".join(c for c in safe if c.isalnum() or c in (' ', '-', '_', '.'))
        return safe[:150]
    
    def _download_file(self, url, titre, subfolder):
        """T√©l√©charge un fichier"""
        try:
            safe_name = self._make_safe_filename(titre or 'document')
            
            # D√©terminer extension depuis URL
            if '.xlsx' in url.lower():
                ext = '.xlsx'
            elif '.xls' in url.lower():
                ext = '.xls'
            elif '.pdf' in url.lower():
                ext = '.pdf'
            elif '.zip' in url.lower():
                ext = '.zip'
            else:
                ext = ''
            
            if not safe_name.endswith(ext) and ext:
                safe_name += ext
            
            filepath = f'{self.output_dir}/{subfolder}/{safe_name}'
            
            # V√©rifier si existe
            if os.path.exists(filepath):
                logging.info(f"    ‚äò D√©j√† t√©l√©charg√©: {safe_name}")
                return filepath
            
            logging.info(f"    ‚Üì T√©l√©chargement: {safe_name}")
            
            response = self.session.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            size_kb = os.path.getsize(filepath) // 1024
            logging.info(f"    ‚úì Sauvegard√© ({size_kb} KB)")
            time.sleep(2)
            
            return filepath
            
        except Exception as e:
            logging.error(f"    ‚úó Erreur t√©l√©chargement: {e}")
            return None
    
    def scrape_all(self):
        """Lance le scraping complet"""
        logging.info("="*60)
        logging.info("SCRAPING HCP - DONN√âES OFFICIELLES")
        logging.info("="*60)
        
        rapport = {
            'date': datetime.now().isoformat(),
            'source': 'HCP - Haut Commissariat au Plan',
            'indicateurs_sociaux': 0,
            'annuaires': 0,
            'publications_sante': 0,
            'publications_specifiques': 0
        }
        
        # 1. Indicateurs Sociaux (Excel annuels)
        logging.info("\n1. T√©l√©chargement Indicateurs Sociaux...")
        indicateurs = self.telecharger_indicateurs_sociaux()
        rapport['indicateurs_sociaux'] = len(indicateurs)
        
        # 2. Annuaire Statistique (Excel)
        logging.info("\n2. T√©l√©chargement Annuaire Statistique...")
        annuaires = self.telecharger_annuaire_statistique()
        rapport['annuaires'] = len(annuaires)
        
        # 3. Explorer page t√©l√©chargements
        logging.info("\n3. Exploration page t√©l√©chargements...")
        publications = self.scrape_page_telechargements()
        rapport['publications_sante'] = len(publications.get('sante', []))
        
        # 4. Publications sp√©cifiques
        logging.info("\n4. T√©l√©chargement publications sp√©cifiques...")
        specifiques = self.telecharger_publications_specifiques()
        rapport['publications_specifiques'] = len(specifiques)
        
        # Sauvegarder rapport
        with open(f'{self.output_dir}/rapport_hcp.json', 'w', encoding='utf-8') as f:
            json.dump(rapport, f, ensure_ascii=False, indent=2)
        
        logging.info("\n" + "="*60)
        logging.info("SCRAPING HCP TERMIN√â!")
        logging.info("="*60)
        logging.info(f"Indicateurs Sociaux: {rapport['indicateurs_sociaux']}")
        logging.info(f"Annuaires: {rapport['annuaires']}")
        logging.info(f"Publications sant√©: {rapport['publications_sante']}")
        logging.info(f"Publications sp√©cifiques: {rapport['publications_specifiques']}")
        logging.info(f"\nR√©sultats dans: {self.output_dir}/")
        logging.info("="*60)
        
        return rapport


if __name__ == "__main__":
    scraper = HCPScraper()
    rapport = scraper.scrape_all()